
\chapter{Related Work\label{ch:pastwork}}

Over the years, different approaches that target very different parts of the
software stack have been studied in the context of software debloating. The key characteristics that distinguish debloating approaches are the following:

\begin{enumerate}
  \item{\textbf{What is being removed and the definition of bloat:} On one hand removing dead code from included libraries and external dependencies has been studied~\cite{jiang2016Jred, jiang2018reddroid, quach2018debloating}. While this approach does not
  target the actual vulnerable parts of the code (e.g., the source of a buffer overflow), it will raise the bar for attackers to exploit such systems by removing gadgets used in Return Oriented Programming (ROP)~\cite{shacham2007geometry}.
  On the other hand, similar to our work, we observe approaches that target the actual features that are unused under certain circumstances~\cite{boomsma2012Dead, heo2018effective, regehr2012CReduce, Snyder2017, sun2018perses}. By defining the bloat as unused code in the applications, as we show later in the results section for web applications, one can mitigate the vulnerable pieces of code and reduce the attack surface of applications significantly.}
  \item{\textbf{Identification of bloat:} The other difference arises from the underlying mechanism used to identify the bloat. Depending on target application and the platform, static analysis might need to be augmented by dynamic analysis to detect
  unused code. In addition to that, the literature has incorporated application configuration files~\cite{Koo:2019:CSD:3301417.3312501}, high level manual specifications~\cite{heo2018effective, sharif2018Trimmer} and usage profiles~\cite{boomsma2012Dead} with dynamic application instrumentation to detect unused code. Contrastingly, Delta Debugging is used~\cite{zeller2002Delta}. This approach is different in that it starts by removing lines of source code and then verifies if the reduction is valid~\cite{heo2018effective, regehr2012CReduce, sun2018perses}.}
  \item{\textbf{Evaluation \& metrics:} Based on the context where debloating is introduced (e.g., To ease the debugging process, reduce the software footprint or reduce the attack surface) different metrics has been used to measure the success of debloating strategies. In the context of security, reduction in size of the target application (i.e., Size of the output binary or number of lines of code), reduction in the number of gadgets available for attackers and removed historical vulnerabilities are measured and reported.}
\end{enumerate}


\subsection{Debloating for the web}
Despite the importance of the web platform, there has been very little work that attempts to apply debloating to it. Snyder et al. investigated the costs and
benefits of giving websites access to all available browser features through
JavaScript~\cite{snyder2017vibrate}. The authors evaluated the use of different
JavaScript APIs in the wild and proposed the use of a client-side extension
which controls which APIs any given website would get access to, depending
on that website's level of trust. Schwarz et al. similarly utilize a browser
extension to limit the attack surface of Chrome and show that they are able
to protect users against microarchitectural and side-channel
attacks~\cite{Schwarz2018}. These studies are orthogonal to our work since
they both focus on the client-side of the web platform, whereas we focus on
the server-side web applications.


%boomsma2012Dead
Boomsma et al. performed dynamic profiling of a custom web application
(a PHP application from an industry partner)~\cite{boomsma2012Dead}. The
authors measured the time it takes for their dynamic profile system to get
complete coverage and the percentage of files that they could remove. Since the
application was a custom one, the authors were not able to report specifics
in terms of the reduction of the programs attack surface, as that relates
to CVEs. Contrastingly, by focusing on popular web applications, and utilizing function-level as well as file-level debloating, we were
able to precisely quantify the reduction of vulnerabilities, both in terms
of known CVEs as well as gadgets for PHP object-injection attacks.

\subsubsection{Most Websites Donâ€™t Need to Vibrate}
This work targets the HTML Standards and browser APIs exposed to JavaScript~\cite{snyder2017vibrate}.
\paragraph{Definition of bloat:} Bloat is defined as the set of browser standards that expose high cost and low benefit. Cost of a standard is defined as a function of historic CVEs affecting the standard, academic papers introducing attacks abusing the standards and number of lines of code in C++, used exclusively to implement the standard. Conversely, the benefit is a function of number of websites that require that feature to function correctly. The judgement of correct functionionality is as perceived by end users while casually browsing through websites.
\paragraph{Identification of bloat:} Based on the cost of each feature, and the number of websites that use that feature, the decision is made to remove individual standards. Through a set of user studies, two users browser top Alexa websites and report if removal of each feature breaks the functionality of these websites. Mapping the JavaScript API to its backend C++ code is a challenging task. To that end, a call graph is generated. By following the path from JavaScript APIs to auto-generated bindings and then to the C++ implementation of each feature, the authors find functions within the source code that exclusively implement each functionality. Figure~\ref{x} depicts the two output of two iterations used to find Battery API functions.
\paragraph{Evaluation \& metrics:} CVE reduction and removed lines of code are the two metrics reported by the authors of this work. Moreover, they propose two modes of debloating: ``Conservative'' and ``Aggressive''. The former tries to keep most number of websites functional and the latter focuses on attack surface reduction. Overall, the results of both modes are promising. They break less websites when compared to NoScript browser extension~\cite{noscript} and have a comparable website break rate to Tor browser.

After identifying the risky and less used features, the JavaScript APIs are removed to prevent websites from accessing them. To that end, a browser extension injects a JavaScript object to proxy the desired objects and their properties such that
calls to removed code fails gracefully. To prevent introducing new vulnerabilities, WebCrypto APIs are whitelisted.

% Limitations: only target preauth

By mapping 1,544 CVEs to 175 web standards within Firefox browser, the authors show that a reduction of 66\% of standards is possible with no noticeable effect on users experience. By preventing untrusted websites from accessing such features the attack surface can be reduced to a great extent.

\subsubsection{Dead Code Elimination for Web Systems Written in PHP: Lessons Learned from an Industry Case}
This work focuses on PHP applications and proposes a system that dynamically detects dead code and eliminates it. The authors goal is to increase the maintainability of the software by eliminating unused files and reducing the total number of files that the developers have to maintain.
\paragraph{Definition of bloat:} Unused code with file level granularity is the target of this work.
\paragraph{Identification of bloat:} Using their system, the authors are able to detect unused files over time. Based on factors such as ``First usage'', ``Number of invocations'', ``Last time used'' and ``Version control last update'', potentially dead files are dynamically discovered and removed. Their approach, that is based on dynamic code coverage faces several inherent challenges. First, some files are rarely used despite not being totally useless. Examples of such files would be scheduled tasks that run monthly or products that are purchased very rarely. In addition to that, error handlers and files under development also receive minimal code coverage from users of the application. As such, a whitelisting mechanism is used in addition to version control system timestamps to detect the last time a file was updated.
\paragraph{Evaluation \& metrics:} With the main forcus of this paper being software engineering, only the file coverage is reported and security implications of using this type of debloating has not been discussed. As the first step in our work, we implement file level debloating and also report on the security effects of this method.

After gathering dynamic code coverage on real deployments of multiple PHP applications used by an industry partner, a reduction of 27\% to 64\% in the number of files is achieved. For different subsystems, it took 1 to 5 months for the code coverage to stabilize, meaning no new files were covered during execution of the application.

\subsection{Debloating in other platforms}

%C-Reduce
%Specific to C/C++
%Target source code
Regehr et al. developed \textit{C-Reduce} which is a tool that works at the source code level~\cite{regehr2012CReduce}.
It performs reduction of C/C++ files by applying very specific program transformation rules.
%Perses
Sun et al. designed a framework called \textit{Perses} that utilizes the grammar of any programming language to guide reduction~\cite{sun2018perses}.
Its advantage is that it does not generate syntactically invalid variants during reduction so that the whole process is made faster.
%Chisel

Heo et al. worked on \textit{Chisel} whose distinguishing feature is that it performs fine-grained debloating by removing code even on the functions that are executed, using reinforcement learning to identify the best reduced program~\cite{heo2018effective}.
%Summary

All three aforementioned approaches are founded on Delta debugging~\cite{zeller2002Delta}.
They reduce the size of an application progressively and verify at each step if the created variant still satisfies the desired properties.

%While these delta debugging reduction techniques could be applied to debloat web applications, it does not scale well for large programs with the usage profiles we record.
%In order to validate a variant, we would need to repeat at each step of the reduction the complete list of the user's interaction with the program to make sure we get the right output and it would be very costly.
%One way to lower the time of each step would be to have a minimal set of features used by a recorded profile so that only those are tested.
%However, most web applications do not provide a set of features to use or a list of APIs to call.
%For this reason, dynamic analysis provides us with a generic way to debloat any web application that is much more efficient and practical for web applications than traditional delta debugging.

%Trimmer
Sharif et al. proposed \textit{Trimmer}, a system that goes further than simple static analysis~\cite{sharif2018Trimmer}.
It propagates the constants that are defined in program arguments and configuration files so that it can remove code that is not used in that particular execution context.
However, their system is not particularly well suited for web applications where we remove complete features.
Our framework goes beyond this contextual analysis by mapping what is actually executed by the application.

Other works include research that revolves mainly around static analysis to remove dead code.
%Java programs
Jiang et al. looked at reducing the bloat of Java applications with a tool called \textit{JRed}~\cite{jiang2016Jred}.
%Android apps
Jiang et al. also designed \textit{RedDroid} to reduce the size of Android applications with program transformations~\cite{jiang2018reddroid}.
%By trimming compile-time and install-time redundancies, the authors reduce the size of Android apps by an average of 15\%.
%Debloating Software through Piece-Wise Compilation and Loading
Quach et al. adopted a different approach by bringing dead-code elimination benefits of static linking to dynamic linking~\cite{quach2018debloating}.
%Shared libraries are pre-built and are not analyzed by the loader at runtime.
%It is then not possible to remove dead code beforehand from these libraries.
%In order to overcome this limitation, the authors generate a metadata file at compile time that then instructs the loader about what should be loaded from these shared libraries.
%These three studies only remove unused code either in the program or in shared libraries. With our system, we remove more than dead code by keeping only the features that are actually used, as quantified by varied usage profiles.

%Cimplifier Container debloating
%rastogi2017Cimplifier
Rastogi et al. looked at debloating a container by partitioning it into smaller and more secure ones~\cite{rastogi2017Cimplifier}. They perform dynamic analysis on system-call logs to determine which components and executables are used in a container, in order to keep them. Koo et al. proposed configuration-driven debloating~\cite{Koo:2019:CSD:3301417.3312501}. Their system removes unused libraries loaded by applications under a specific configuration. They test their system on Nginx, VSFTPD, and OpenSSH and show a reduction of 78\% of code from Nginx libraries is possible based on specific configurations.

\subsubsection{Simplifying and Isolating Failure-Inducing Input}
One line of work in the literature of software debloating is based on Delta Debugging algorithm. This idea was first introduced by Zeller et. al. in 2002~\cite{zeller2002Delta}.
Given a failure inducing piece of code as input, Delta Debugging automatically minimizes the size of the test case by searching the program space for smaller pieces of code that would still cause the target application to run into error.

\paragraph{Definition of bloat:} Under DD, bloat is defined as extra code in a failure inducing input, such that their removal does not affect the outcome of the execution of the program, which is to throw an error in this case.
\paragraph{Identification of bloat:} The input is split into chunks of an arbitrary size n. Then the algorithm verifies if the first chunk or the n-1 chunks would cause the target application to crash. If neither would satisfy this goal, n is increased and the algorithm iterates using smaller chunks until we can isolate the part where the cause of the error lies. This way, we will end up with a minimal piece of code satisfying our target.
\paragraph{Evaluation \& metrics:} Minimizing the number of lines of input is the target of this algorithm and the performance is measured as the execution time to find the minimal input.

Without any prior knowledge of the input program and syntax, Delta Debugging has shown to effectively find minimal inputs on real bug reports. In their first case study, DD is able to reduce an input that would crash Firefox browser from 95 actions ot 3 actions (openning the print dialogue and mouse down and up events on print button), and a further reduction from 896 to 1 line for the html input to be printed. Ultimately, the bug report is summarized as ``Printing a page with a SELECT tag would cause the browser to crash''. The execution time to minimize this bug report is 35 minutes on a 500 MHz PC requiring 139 rounds. Overall, DD is able to reduce failure inducing programs in polynomial time with respect to the input.

\subsubsection{Effective Program Debloating via Reinforcement Learning}
Since the introduction of Delta Debugging, researchers have proposed optimizations that will enable this algorithms to terminate faster by taking into account the characteristics of the target language such as program's grammar. In this paper, Hoe et. al. introduced reinforcement learning as a means to intelligently and efficiently interpret input code's characteristics and reduce it respectively~\cite{heo2018effective}. In contrast to original Delta Debugging paper, this work introduces Chisel, a tool that applies software minimization techniques with the goal of attack surface reduction.

\paragraph{Definition of bloat:} Given a high level specification of the required functionalities of an application, Chisel aims at removing unnecessary features. This configuration is defined as a test cases with a set of inputs and desired outputs for the application.
\paragraph{Identification of bloat:} By applying Delta Debugging and rerunning the test cases, Chisel searches for the minimal program that satisfies the test requirements.
\paragraph{Evaluation \& metrics:} Lines of code reduction as well as removed known vulnerabilities are reported.

The main contribution of Chisel is to intelligently drop chunks of code from the program rather than doing a blind binary search. When compared to C-Reduce and Perses, Chisel is able to debloat real world applications (Linux binaries) in a timely manner whereas the other algorithms would not terminate. Overall, for ten GNU CoreUtil binaries, Chisel was able to reduce 90\% of lines of code while keeping the functionality similar to BusyBox, which is an optimized version of GNU CoreUtils with minimal options targetted for embedded systems.

Chisel comes with two inherent drawbacks. First, by randomly removing parts of real applications, we will remove code that does not add to the functionality of the program, but is handling tasks such as error handling and security checks.
For example, Chisel can remove a boundary check or a stack canary, as the target application would still function correctly without them. To address this issue, the authors relied on fuzzing and static code analysis techniques to pinpoint some of these
security checks and prevent them from being removed.

Secondly, the generation of high level program specifications is a non-trivial task. In the example of ``tar'' archive software discussed in the paper, only a set of simple compression and decompression tests are performed. This approach can potentially open the door to new and hidden vulnerabilities, introduced after debloating.

\subsubsection{Shachee}

\subsubsection{Config Driven}
