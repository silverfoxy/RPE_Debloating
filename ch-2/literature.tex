
\chapter{Related Work\label{ch:pastwork}}

Over the years, different approaches that target very different parts of the
software stack have been studied in the context of software debloating. The key characteristics that distinguish debloating approaches are the following:

\begin{enumerate}
  \item{\textbf{What is being removed and the definition of bloat:} On one hand, prior work has proposed systems for removing dead code from included libraries and external dependencies ~\cite{jiang2016Jred, jiang2018reddroid, quach2018debloating}. While this approach does not
  target the actual vulnerable parts of the code (e.g., the source of a buffer overflow), it does raise the bar for attackers to exploit such systems by removing gadgets used in Return Oriented Programming (ROP)~\cite{shacham2007geometry}.
  On the other hand, similar to our work, there are approaches that target the actual features that are unused under certain usage scenarios~\cite{boomsma2012Dead, heo2018effective, regehr2012CReduce, Snyder2017, sun2018perses}. By defining ``bloat'' as unused code in software applications, one can remove the vulnerable pieces of code that are unused, and significantly reduce the attack surface of applications.}
  \item{\textbf{Identification of bloat:} The other difference arises from the underlying mechanism used to identify the bloat. Depending on the target application and the platform, static analysis might need to be augmented by dynamic analysis to detect
  unused code. In addition to that, prior work has incorporated application-configuration files~\cite{Koo:2019:CSD:3301417.3312501}, high-level, manual specifications~\cite{heo2018effective, sharif2018Trimmer} and usage profiles~\cite{boomsma2012Dead} with dynamic application instrumentation to detect unused code. Delta Debugging~\cite{zeller2002Delta} is an alternative approach where one starts by removing lines of source code and then verify if the reduction is valid~\cite{heo2018effective, regehr2012CReduce, sun2018perses}.}
  \item{\textbf{Evaluation \& metrics:} Based on the context where debloating is introduced (e.g., To ease the debugging process, reduce the software footprint, or reduce the attack surface) different metrics has been used to measure the success of debloating strategies. In the context of security, researchers have measured and reported the reduction in size of the target application (i.e., Size of the output binary or number of lines of code), the reduction in the number of gadgets available for attackers, and the number of removed historical vulnerabilities and exploits.}
\end{enumerate}


\subsection{Debloating for the web}
Despite the importance of the web platform, there has been very little work that attempts to apply debloating to it. Snyder et al. investigated the costs and
benefits of giving websites access to all available browser features through
JavaScript~\cite{snyder2017vibrate}. The authors evaluated the use of different
JavaScript APIs in the wild and proposed the use of a client-side extension
which controls which APIs any given website would get access to, depending
on that website's level of trust. Schwarz et al. similarly utilize a browser
extension to limit the attack surface of Chrome and show that they are able
to protect users against microarchitectural and side-channel
attacks~\cite{Schwarz2018}. 
%These studies are orthogonal to our work since
%they both focus on the client-side of the web platform, whereas we focus on
%the server-side web applications.


%boomsma2012Dead
Boomsma et al. performed dynamic profiling of a custom web application
(a PHP application from an industry partner)~\cite{boomsma2012Dead}. The
authors measured the time it takes for their dynamic profile system to get
complete coverage and the percentage of files that they could remove. Since the
application was a custom one, the authors were not able to report specifics
in terms of the reduction of the programs attack surface, as that relates
to CVEs. 

%Contrastingly, by focusing on popular web applications, and utilizing function-level as well as file-level debloating, we were
%able to precisely quantify the reduction of vulnerabilities, both in terms
%of known CVEs as well as gadgets for PHP object-injection attacks.

We discuss the work by Snyder et al.~\cite{snyder2017vibrate} and Boosma et al.~\cite{boomsma2012Dead} in more detail below:

\subsubsection{Most Websites Don't Need to Vibrate}
This work targets the HTML Standards and browser APIs exposed to JavaScript~\cite{snyder2017vibrate}.
\paragraph{Definition of bloat:} Bloat is defined as the set of browser standards that expose high cost and low benefit. Cost of a standard is defined as a function of historic CVEs affecting the standard, academic papers introducing attacks abusing the standard and number of lines of code in C++, used exclusively to implement the standard. Conversely, the benefit is a function of number of websites that require that feature to function correctly. The judgement of correct functionionality is as perceived by end users while casually browsing through websites.
\paragraph{Identification of bloat:} Based on the cost of each feature, and the number of websites that use that feature, a decision is made to remove individual standards. Through a set of user studies, two users browser top Alexa websites and report if removal of each feature breaks the functionality of these websites.

Mapping the JavaScript API to its backend C++ code is a challenging task. A call graph is generated to extract that information. By following the path from JavaScript APIs to auto-generated bindings and then to the C++ implementation of each feature, the authors find functions within the source code that exclusively implement each functionality.
 \paragraph{Evaluation \& metrics:} CVE reduction and removed lines of code are the two metrics reported by the authors of this work. Moreover, they propose two modes of debloating: ``Conservative'' and ``Aggressive''. The former tries to keep most number of websites functional and the latter focuses on attack surface reduction. Overall, the results of both modes are promising. They break less websites in comparison with NoScript browser extension~\cite{noscript} and have a comparable website breakage rate to the Tor browser.

After identifying the risky and less used features, the authors remove specific JavaScript APIs thereby preventing websites from accessing them. The APIs are removed through a browser extension that injects a JavaScript proxy object to modify target APIs and their properties such that
calls to removed code fail gracefully. Additionaly, to avoid introducing new vulnerabilities due to blocked security-related functions, the authors whitelist the WebCrypto APIs.

% Limitations: only target preauth

By mapping 1,544 CVEs to 175 web standards within Firefox browser, the authors show that a reduction of 66\% of standards is possible with no noticeable effect on user experience. By preventing untrusted websites from accessing such features the attack surface can be reduced significantly.

\subsubsection{Dead Code Elimination for Web Systems Written in PHP: Lessons Learned from an Industry Case}
This work focuses on PHP applications and proposes a system that dynamically detects dead code and eliminates it. The author's goal is to increase the maintainability of the software by eliminating unused files and reducing the total number of files that the developers have to maintain.
\paragraph{Definition of bloat:} Bloat is defined as files that are not executed by users during their normal use of the application.
\paragraph{Identification of bloat:} Using their system, the authors are able to detect unused files over time. Based on factors such as ``First usage'', ``Number of invocations'', ``Last time used'' and ``Version control last update'', potentially dead files are dynamically discovered and removed. Their approach is based on dynamic code coverage and faces several inherent challenges. First, even though some files are rarely used this does not mean that they are not useful. Examples of such files would be scheduled tasks that run monthly or products that are purchased very rarely. In addition to that, error handlers and files under development also receive minimal code coverage from users of the application. To deal with these issues, the authors use a whitelisting mechanism in addition to version control system timestamps to detect the last time a file was updated and prevent the removal of features under active development.

\paragraph{Evaluation \& metrics:} Since the main focus of this paper is related to software engineering metrics, the authors only report on file coverage and do not discuss the implications of debloating to security. %As the first step in our work, we implement file level debloating and also report on the security effects of this method.
Specifically, after gathering dynamic code coverage on real deployments of multiple PHP applications used by an industry partner, the authors report a reduction of 27\% to 64\% in the number of files. For different subsystems, it took 1 to 5 months for the code coverage to stabilize, meaning no new files were executed after that period.

\subsection{Debloating in other platforms}

%C-Reduce
%Specific to C/C++
%Target source code
Regehr et al. developed \textit{C-Reduce} which is a tool that works at the source code level~\cite{regehr2012CReduce}.
It performs reduction of C/C++ files by applying very specific program transformation rules.
%Perses
Sun et al. designed a framework called \textit{Perses} that utilizes the grammar of any programming language to guide reduction~\cite{sun2018perses}.
Its advantage is that it does not generate syntactically invalid variants during reduction so that the whole process is made faster.
%Chisel

Heo et al. worked on \textit{Chisel} whose distinguishing feature is that it performs fine-grained debloating by removing code even on the functions that are executed, using reinforcement learning to identify the best reduced program~\cite{heo2018effective}.
%Summary

All three aforementioned approaches are founded on Delta debugging~\cite{zeller2002Delta}.
They reduce the size of an application progressively and verify at each step if the created variant still satisfies the desired properties.

%While these delta debugging reduction techniques could be applied to debloat web applications, it does not scale well for large programs with the usage profiles we record.
%In order to validate a variant, we would need to repeat at each step of the reduction the complete list of the user's interaction with the program to make sure we get the right output and it would be very costly.
%One way to lower the time of each step would be to have a minimal set of features used by a recorded profile so that only those are tested.
%However, most web applications do not provide a set of features to use or a list of APIs to call.
%For this reason, dynamic analysis provides us with a generic way to debloat any web application that is much more efficient and practical for web applications than traditional delta debugging.

%Trimmer
Sharif et al. proposed \textit{Trimmer}, a system that goes further than simple static analysis~\cite{sharif2018Trimmer}.
It propagates the constants that are defined in program arguments and configuration files so that it can remove code that is not used in that particular execution context.
However, their system is not particularly well suited for web applications where we remove complete features.
Our framework goes beyond this contextual analysis by mapping what is actually executed by the application.

Other works include research that revolves mainly around static analysis to remove dead code.
%Java programs
Jiang et al. looked at reducing the bloat of Java applications with a tool called \textit{JRed}~\cite{jiang2016Jred}.
%Android apps
Jiang et al. also designed \textit{RedDroid} to reduce the size of Android applications with program transformations~\cite{jiang2018reddroid}.
%By trimming compile-time and install-time redundancies, the authors reduce the size of Android apps by an average of 15\%.
%Debloating Software through Piece-Wise Compilation and Loading
Quach et al. adopted a different approach by bringing dead-code elimination benefits of static linking to dynamic linking~\cite{quach2018debloating}.
%Shared libraries are pre-built and are not analyzed by the loader at runtime.
%It is then not possible to remove dead code beforehand from these libraries.
%In order to overcome this limitation, the authors generate a metadata file at compile time that then instructs the loader about what should be loaded from these shared libraries.
%These three studies only remove unused code either in the program or in shared libraries. With our system, we remove more than dead code by keeping only the features that are actually used, as quantified by varied usage profiles.

%Cimplifier Container debloating
%rastogi2017Cimplifier
Rastogi et al. looked at debloating a container by partitioning it into smaller and more secure ones~\cite{rastogi2017Cimplifier}. They perform dynamic analysis on system-call logs to determine which components and executables are used in a container, in order to keep them. Koo et al. proposed configuration-driven debloating~\cite{Koo:2019:CSD:3301417.3312501}. Their system removes unused libraries loaded by applications under a specific configuration. They test their system on Nginx, VSFTPD, and OpenSSH and show a reduction of 78\% of code from Nginx libraries is possible based on specific configurations.

Next, we discuss three instances of related work that are representative of the categories of debloating approaches.

\subsubsection{Simplifying and Isolating Failure-Inducing Input}
One line of work in the literature of software debloating is based on the Delta Debugging algorithm. This idea was first introduced by Zeller et. al. in 2002~\cite{zeller2002Delta}.
Given a failure inducing piece of code as input, Delta Debugging automatically minimizes the size of the test case by searching the program space for smaller pieces of code that would still cause the target application to run into error.

\paragraph{Definition of bloat:} Under Delta Debugging, bloat is defined as extra code in a failure inducing input, such that its removal does not affect the outcome of the execution of the program which, in this case, is to throw an error.

\paragraph{Identification of bloat:} The input is split into chunks of an arbitrary size n. Then the algorithm verifies if the first chunk or the n-1 chunks would cause the target application to crash. If neither would satisfy this goal, n is increased and the algorithm iterates using smaller chunks until it can isolate the part where the cause of the error lies. This way, we will end up with a minimal piece of code that still breaks the target test case.
\paragraph{Evaluation \& metrics:} Minimizing the number of lines of input is the target of this algorithm. The performance of this system is measured as the execution time required to find the minimal input.

Without any prior knowledge of the input program and syntax, Delta Debugging has shown to effectively find minimal inputs on real bug reports. In their first case study, DD is able to reduce an input that would crash Firefox browser from 95 actions to 3 actions (openning the print dialogue and mouse down and up events on print button), and a further reduction from 896 to 1 line for the html input to be printed. Overall, DD is able to reduce failure inducing programs in polynomial time with respect to size of the input.

\subsubsection{Effective Program Debloating via Reinforcement Learning}
Since the introduction of Delta Debugging, researchers have proposed optimizations that will enable this algorithm to terminate faster by taking into account the characteristics of the target language such as program's grammar. Hoe et. al. proposed Chisel, a tool that applies software minimization techniques with the goal of attack surface reduction~\cite{heo2018effective}. Chisel uses reinforcement learning as a means to intelligently and efficiently interpret input code's characteristics and subsequently reduce it. 

\paragraph{Definition of bloat:} Given a high level specification of the required functionalities of an application, Chisel aims at removing unnecessary features. This configuration is defined as a set of test cases with inputs and desired outputs for the application after each execution.
\paragraph{Identification of bloat:} By applying Delta Debugging and rerunning the test cases, Chisel searches for the minimal program that satisfies the test requirements.
\paragraph{Evaluation \& metrics:} The authors evaluate Chisel based on the lines of code reduction and the removal of known vulnerabilities.

The main contribution of Chisel is to intelligently drop chunks of code from the program rather than doing a blind binary search. When compared to C-Reduce and Perses, Chisel is able to debloat real world applications (Linux binaries) in a timely manner whereas the other algorithms would not terminate. Overall, for ten GNU CoreUtil binaries, Chisel was able to reduce 90\% of lines of code while keeping the functionality similar to BusyBox, which is an optimized version of GNU CoreUtils with minimal options targetted for embedded systems.

Chisel comes with two inherent drawbacks. First, by randomly removing parts of real applications, it may remove code that does not add to the functionality of the program, but is handling tasks such as error handling and security checks.
For example, Chisel can remove a boundary check or a stack canary, as the target application would still function correctly without them. To address this issue, the authors relied on fuzzing and static code analysis techniques to pinpoint some of these
security checks and prevent them from being removed.

Second, the generation of high level program specifications is a non-trivial task. In the example of the ``tar'' archive software discussed in the paper, only a set of simple compression and decompression tests were performed. \todo{How is this different than the potential errors discussed in the previous paragraph?} This approach can potentially open the door to new and hidden vulnerabilities, introduced after debloating.

\subsubsection{Shredder: Breaking Exploits through API Specialization}
Mishra et. al. presented an API specialization scheme for closed source binaries~\cite{mishra2018shredder}. The main idea is to characterize how legitimate applications use sensitive APIs compared to exploit attempts. By generating policies that
whitelist legitimate use of these APIs, they can break more than 86\% of existing shell codes for 10 Windows applications in their dataset and stop all ROP exploits for 8/10 applications.

\paragraph{Definition of bloat:} Instead of only focusing on unused code, they define policies to limit the set of parameters passed to the APIs.
\paragraph{Identification of bloat:} Based on static analysis and backward propagation from sensitive APIs, the authors are able to find call sites and move backwards to extract possible values for parameters passed to these functions wherever possible. This approach is not able to identify values passed to every API call across the application due to limitations of static analysis. But this best effort approach can be coupled with control flow integrity to prevent calls to the sensitive APIs from unseen locations during an exploit attempt.
\paragraph{Evaluation \& metrics:} The implementation that the authors chose for their API specialization, does not actually remove unused code but, by enforcing policies, it limits possible invocation of sensitive APIs. In principle, this method can be coupled with other debloating strategies to remove the corresponding code. As such, the authors report the number of broken shell codes before and after applying the policies.

API specialization tries to distinguish the different characteristics of benign and malicious execution of the program. The same idea can be applied to web applications, but static analysis is more limited on dynamic web programming languages such as PHP. Therefore, a combination of dynamic usage profiling and API specialization can be used as an extra step to further debloat a web application after source code debloating.
